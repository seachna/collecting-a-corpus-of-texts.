# Лабораторная работа №1 — Сбор корпуса текстов
**Дисциплина:** Теория информации  
**Факультет:** Компьютерных наук и информационных технологий  
**Кафедра:** Теоретических основ компьютерной безопасности и криптографии  
**Студент:** Чуйкина Алёна Сергеевна  
**Группа:** 331  
**Направление:** 10.05.01 – Компьютерная безопасность  
**Преподаватель:** А.А. Лобов  
**Дата:** 16.10.2025  
**Университет:** Саратовский национальный исследовательский государственный университет имени Н.Г. Чернышевского  

## Тема работы
**Сбор корпуса текстов** — источник: страницы Википедии о российских городах.  
Цель: сформировать корпус объёмом не менее **64 МБ** и рассчитать его характеристики (размер, число символов и слов, TTR, энтропия, сжатие и т.д.).

## Краткое описание
Проект автоматизирует следующие этапы:
1. Получение списка ссылок на статьи о городах России.  
2. Пакетное скачивание статей и извлечение основного текста.  
3. Сохранение каждого текста в отдельный файл в `cities/`.  
4. Объединение всех файлов в `corpus_cities_raw.txt`.  
5. Подсчёт статистик корпуса с помощью `corpus_stats_full.py`.

## Структура проекта 
lab1_corpus/
├─ get_city_links.py
├─ download_city_texts.py
├─ corpus_stats_full.py
├─ README.md
├─ cities/                 
├─ corpus_cities_raw.txt    

## Система программирования
- Язык: **Python 3.12**  
- ОС: **Windows 11**  
- Среда разработки: **IDLE** (или любой редактор)  
- Библиотеки:
python -m pip install --user requests beautifulsoup4

## Примеры запуска программ 
### 1) Получить список ссылок
python get_city_links.py
**Результат:** файл `city_links.txt` — список URL статей (по одному в строке).
### 2) Скачать статьи и извлечь текст
python download_city_texts.py
**Результат:** папка `cities/` с файлами `0001_Название.txt`, `0002_...` и т.д.
### 3) Объединить все файлы в один корпус
cat cities/*.txt > corpus_cities_raw.txt
**Результат:** `corpus_cities_raw.txt` 
### 4) Посчитать статистику корпуса
python corpus_stats_full.py > stats_report.txt
**Результат:** программа выведет в консоль и/или в `stats_report.txt` параметры корпуса:
* Bytes (байты и MB)
* Chars (символы)
* Tokens (слов) и Types (уникальные)
* TTR = types / tokens
* Top-50 слов
* Shannon entropy (bits/char)
* Gzip size (оценка сжатия)
## Вывод 
Bytes: 91363558 ( 87.13107872009277 MB )
Chars: 51249608 Chars (no spaces): 43370202
Tokens: 6798829 Types: 291839 TTR: 0.0429
Top 50 words: [('в', 229431), ('и', 152092), ('года', 111531), ('на', 94140), ('с', 74938), ('1', 73392), ('по', 56932), ('2', 46947), ('архивировано', 44224), ('дата', 43096), ('населения', 43017), ('обращения', 42876), ('3', 34028), ('году', 33331), ('города', 29506), ('5', 28836), ('из', 27973), ('а', 26779), ('января', 26501), ('4', 25880), ('российской', 24929), ('от', 24512), ('численность', 24285), ('федерации', 22706), ('россии', 20400), ('город', 19871), ('области', 19525), ('м', 19460), ('6', 19238), ('к', 19104), ('7', 17760), ('г', 16646), ('8', 16128), ('рус', 15298), ('9', 14678), ('10', 14345), ('2020', 14145), ('городских', 13825), ('октября', 13820), ('муниципальным', 13688), ('до', 13264), ('образованиям', 13259), ('о', 13217), ('был', 13164), ('2019', 13118), ('2018', 12210), ('2016', 12204), ('2021', 12150), ('мая', 11677), ('июля', 11611)]
Shannon entropy (bits/char): 5.226
Gzip size (bytes): 21799335 ( 20.78946590423584 MB )




# Лабораторная работа №2 — Статистические особенности языка  
**Дисциплина:** Теория информации  
**Факультет:** Компьютерных наук и информационных технологий  
**Кафедра:** Теоретических основ компьютерной безопасности и криптографии  
**Студент:** Чуйкина Алёна Сергеевна  
**Группа:** 331  
**Направление:** 10.05.01 – Компьютерная безопасность  
**Преподаватель:** А.А. Лобов  
**Дата:** 26.10.2025  
**Университет:** Саратовский национальный исследовательский государственный университет имени Н.Г. Чернышевского  

## Тема работы
**Статистические особенности языка.**  
Цель: исследовать закономерности русского языка с помощью цепей Маркова и рассчитать вероятности появления символов после предыдущих `k` символов (для `k = 0..16`), а также вычислить условную энтропию.

## Краткое описание
Программа анализирует текстовый корпус (`corpus_cities_raw.txt`, полученный в ЛР №1) и вычисляет:
- частотные распределения символов (`k=0`);
- условные вероятности появления символов после `k` предыдущих (для `k=1..16`);
- условную энтропию для каждого порядка `k`;
- формирует отчёты и сохраняет результаты в папку `outputs/`.

## Структура проекта
lab2_markov/
├─ markov_stats.py           
├─ corpus_cities_raw.txt      
├─ outputs/                     # папка с результатами
│   ├─ prob_k0.csv
│   ├─ prob_k1.csv
│   ├─ ...
│   ├─ summary_k16.txt
│   └─ overall_summary.txt
├─ README.md

## Система программирования
- **Язык:** Python 3.12  
- **ОС:** Windows 11  
- **Среда разработки:** IDLE / Visual Studio Code / Git Bash  
- **Библиотеки:** стандартные (`collections`, `math`, `csv`, `argparse`, `pathlib`, `re`, `sys`)  
- **Дополнительные пакеты не требуются**

## Алгоритм работы программы
1. Прочитать корпус текстов `corpus_cities_raw.txt`.  
2. Нормализовать текст: перевести в нижний регистр, оставить только буквы русского алфавита, пробел и знаки `. , ! ?`.  
3. Для `k=0` — посчитать частоты отдельных символов.  
4. Для каждого `k=1..16`:
   - сформировать все пары `(context, next)` — контекст длиной `k` и следующий символ;  
   - посчитать количество вхождений каждой пары;  
   - вычислить условные вероятности `P(next|context)`;  
   - записать результаты в файлы `prob_k{k}.csv` и `summary_k{k}.txt`.  
5. Вычислить условную энтропию:
6. Сохранить общую сводку (`overall_summary.txt`).

## Пример запуска программы
python markov_stats.py --corpus corpus_cities_raw.txt 

## Пример результатов работы

**Файл `summary_k1.txt`:**
k = 1
Total transitions counted: 45243558
Number of contexts (unique): 38
Conditional entropy H(next|context): 3.623519 bits

Top contexts by total count (up to 20):
Context ' ' (6221470 occurrences): top next -> 'п':551775(0.089), 'с':537175(0.086), 'в':492788(0.079), 'г':429616(0.069), 'о':386114(0.062)
Context 'о' (4202217 occurrences): top next -> ' ':515803(0.123), 'в':507387(0.121), 'д':458900(0.109), 'р':395427(0.094), 'с':390177(0.093)
Context 'а' (3290253 occurrences): top next -> ' ':653326(0.199), 'н':356068(0.108), 'л':263021(0.080), 'р':253236(0.077), 'с':214275(0.065)
Context 'и' (2977156 occurrences): top next -> ' ':499721(0.168), 'я':264203(0.089), 'н':227361(0.076), 'й':206970(0.070), 'с':170870(0.057)
Context 'е' (2790166 occurrences): top next -> 'н':489700(0.176), ' ':432200(0.155), 'р':316687(0.114), 'л':277963(0.100), 'с':186947(0.067)
Context 'н' (2594848 occurrences): top next -> 'и':506253(0.195), 'о':505412(0.195), 'а':445466(0.172), 'ы':264574(0.102), 'н':170348(0.066)
Context 'с' (2424497 occurrences): top next -> 'т':594023(0.245), 'к':512664(0.211), 'е':187778(0.077), 'о':147970(0.061), 'с':142320(0.059)
Context 'р' (2376383 occurrences): top next -> 'о':551001(0.232), 'а':492374(0.207), 'е':279199(0.117), 'и':210787(0.089), 'у':116941(0.049)
Context 'т' (1873276 occurrences): top next -> 'а':304430(0.163), 'о':274538(0.147), 'е':206041(0.110), 'и':194970(0.104), 'р':163952(0.088)
Context 'в' (1726048 occurrences): top next -> ' ':329151(0.191), 'а':288046(0.167), 'о':269475(0.156), 'е':184877(0.107), 'и':132208(0.077)
Context 'к' (1524012 occurrences): top next -> 'о':446628(0.293), 'а':275935(0.181), 'и':266877(0.175), ' ':118421(0.078), 'р':98327(0.065)
Context 'л' (1468121 occurrences): top next -> 'е':299400(0.204), 'ь':265333(0.181), 'и':194447(0.132), 'а':185353(0.126), 'о':174504(0.119)
Context 'д' (1179634 occurrences): top next -> 'а':299888(0.254), 'е':177394(0.150), 'о':132963(0.113), 'и':98796(0.084), 'с':73300(0.062)
Context 'м' (979376 occurrences): top next -> ' ':207370(0.212), 'е':135691(0.139), 'и':129522(0.132), 'а':129282(0.132), 'о':104962(0.107)
Context 'я' (954983 occurrences): top next -> ' ':605386(0.634), 'н':62149(0.065), 'т':46747(0.049), 'б':42362(0.044), 'м':26394(0.028)
Context 'г' (903383 occurrences): top next -> 'о':541829(0.600), 'а':68820(0.076), 'и':68152(0.075), 'р':56189(0.062), 'е':41718(0.046)
Context 'п' (896037 occurrences): top next -> 'о':289988(0.324), 'р':219041(0.244), 'е':108514(0.121), 'а':91513(0.102), 'и':49051(0.055)
Context 'у' (794267 occurrences): top next -> ' ':99564(0.125), 'с':86964(0.109), 'р':81344(0.102), 'н':54687(0.069), 'л':46034(0.058)
Context '.' (748938 occurrences): top next -> ' ':735547(0.982), ',':7003(0.009), 'р':1141(0.002), '.':1053(0.001), 'с':471(0.001)
Context 'й' (651752 occurrences): top next -> ' ':458727(0.704), 'с':81469(0.125), 'о':27379(0.042), ',':17994(0.028), 'н':17829(0.027)

**Фрагмент файла `prob_k1.csv`:**
context,next,count,probability
а,б,55294,0.016805
а,з,116456,0.035394
а, ,653326,0.198564
а,к,75696,0.023006
а,с,214275,0.065124
а,в,153129,0.046540
а,д,91117,0.027693
а,т,203630,0.061889
а,л,263021,0.079939
а,н,356068,0.108219
а,ч,33576,0.010205
а,ф,17972,0.005462
а,м,101594,0.030877
а,ц,59612,0.018118
а,щ,48591,0.014768
а,е,33253,0.010107
а,п,36235,0.011013
а,ю,17233,0.005238
а,",",34346,0.010439
а,г,32589,0.009905
а,я,167426,0.050885
а,.,91148,0.027702
а,х,45287,0.013764
а,р,253236,0.076966
а,у,12995,0.003950
а,ш,18222,0.005538
а,й,60238,0.018308
а,о,6809,0.002069
а,и,13085,0.003977
а,ж,19257,0.005853
а,э,2924,0.000889
а,а,882,0.000268
а,?,204,0.000062
а,ё,1357,0.000412
а,!,155,0.000047
а,ь,14,0.000004
а,ы,1,0.000000

## Результаты вычисления характеристик
* Для `k=1` энтропия ≈ **3.623519 bits**
* Для `k=16` энтропия снизилась до **0.182599 bits**, что отражает рост предсказуемости текста при учёте контекста.
* Наиболее частыми контекстами являются типичные сочетания русских букв и пробелов.
* Результаты подтверждают, что язык обладает высокой степенью структурированности и статистической зависимости между символами.

## Вывод

В ходе лабораторной работы:
* Реализована программа для анализа статистических особенностей текста с использованием цепей Маркова.
* Вычислены вероятности появления символов и условные энтропии для различных порядков `k`.
* Продемонстрировано снижение энтропии при увеличении длины контекста, что соответствует закономерностям естественного языка.
* Все результаты сохранены в папке `outputs/` и загружены в репозиторий GitHub.